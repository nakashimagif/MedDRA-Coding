{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initilization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# coding:utf-8\n",
    "%matplotlib inline\n",
    "#import sympy\n",
    "import scipy\n",
    "# init_session()\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy.random import *\n",
    "import itertools\n",
    "from itertools import chain, permutations\n",
    "import os\n",
    "import pickle\n",
    "import random\n",
    "seed(1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import read_data\n",
    "df = read_data.read_data()\n",
    "\n",
    "\n",
    "def is_ascii(string):\n",
    "    \"\"\"return true if non ascii characters are detected in the given string\n",
    "    \"\"\"\n",
    "    if string:\n",
    "        return max([ord(char) for char in string]) < 128\n",
    "    return True\n",
    "\n",
    "\n",
    "def texts2list(texts):\n",
    "    stop_words = set('for a of the and to in by with'.split())\n",
    "    if type(texts) == list:\n",
    "        texts = pd.Series(texts)\n",
    "    texts = texts.str.lower().str.translate(str.maketrans('()', '  '))\n",
    "    texts = texts.str.strip()\n",
    "    textlist = [[\n",
    "        word for word in document.translate(\n",
    "            str.maketrans('\\'()[],.&?\"{}-_:;', '                ')).split()\n",
    "        if word not in stop_words\n",
    "    ] for document in texts]\n",
    "    return textlist\n",
    "\n",
    "\n",
    "#Select AE reported in English\n",
    "df = df[df['AEDECOD'].apply(is_ascii)]\n",
    "df['AETERM'] = df['AETERM'].str.lower()\n",
    "#drop duplicate\n",
    "df.drop_duplicates(inplace=True)\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "#create coder and decoder\n",
    "pt = df['AEDECOD'].drop_duplicates()\n",
    "coder = {}\n",
    "for i, word in enumerate(pt):\n",
    "    coder.update({word: i})\n",
    "    i = i + 1\n",
    "decoder = dict([(pt, index) for index, pt in coder.items()])\n",
    "dest = os.path.join('.', 'data', 'coder.pkl')\n",
    "pickle.dump(coder, open(dest, 'wb'), protocol=4)\n",
    "\n",
    "\n",
    "def count_aeterm(df):\n",
    "    texts = texts2list(df['AETERM'])\n",
    "    check = [len(text) for text in texts]\n",
    "    df['len'] = check\n",
    "    print('distribution of number of words per AETERM')\n",
    "    print('data\\n', df['len'].value_counts().sort_index())\n",
    "\n",
    "\n",
    "count_aeterm(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.models import Model, Sequential\n",
    "from keras.engine import Input\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.layers import Dense, Input, LSTM, Activation, Dropout\n",
    "from keras.optimizers import SGD\n",
    "from keras.callbacks import EarlyStopping, CSVLogger\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "wikimodel = Word2Vec.load(\"./data/wiki.en.word2vec.model\")\n",
    "wvsize = wikimodel.vector_size\n",
    "MAX_WORDS = 5\n",
    "\n",
    "texts = texts2list(df['AETERM'])\n",
    "dictset = list(set(chain.from_iterable(texts)))\n",
    "wvdict = dict(\n",
    "    [(dic, wikimodel[dic]) for dic in dictset if dic in wikimodel.wv.vocab])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def texts2wv(texts):\n",
    "    wv = [\n",
    "        np.array([wvdict[word] for word in text if word in wvdict.keys()])\n",
    "        for text in texts\n",
    "    ]\n",
    "    nonull = [len(vector) for vector in wv]\n",
    "    x = [\n",
    "        np.array([\n",
    "            wvdict[word] for (i, word) in enumerate(text)\n",
    "            if word in wvdict.keys()\n",
    "        ]) for text in texts\n",
    "    ]\n",
    "    x_norm = [x[i][0:MAX_WORDS] for i in range(len(x))]\n",
    "    x_all = np.array([\n",
    "        np.append(x_norm[i].flat,\n",
    "                  np.zeros(wvsize * max(MAX_WORDS - len(x_norm[i]), 0)))\n",
    "        for i in range(len(x_norm))\n",
    "    ])\n",
    "    return x_all, nonull\n",
    "\n",
    "\n",
    "texts = texts2list(df['AETERM'])\n",
    "x_all, nonnull = texts2wv(texts)\n",
    "df['nonnull'] = nonnull\n",
    "df = df[df.nonnull > 0]\n",
    "df_all = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sizeflag = df_all.groupby(\"AEDECOD\").size().reset_index()\n",
    "sizeflag.columns = ['AEDECOD', 'AEDECOD_SIZE']\n",
    "df = pd.merge(df_all, sizeflag, on=\"AEDECOD\", how=\"left\")\n",
    "\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Select AEDECOD which have 2 or more record\n",
    "df = df[df['AEDECOD_SIZE'] >= 2]\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "print(\"Unique PT(#>=1): \", len(df_all[~df_all['AEDECOD'].duplicated()]))\n",
    "print(\"Unique PT(#>=2): \", len(df[~df['AEDECOD'].duplicated()]))\n",
    "\n",
    "\n",
    "def sampling(groupdf):\n",
    "    groupdf['target'] = np.append(\n",
    "        permutation(2), binomial(n=1, p=0.8, size=len(groupdf) - 2))\n",
    "    return groupdf\n",
    "\n",
    "\n",
    "# Data Split\n",
    "df = df.groupby(\"AEDECOD\").apply(sampling)\n",
    "\n",
    "print('All Record')\n",
    "count_aeterm(df_all)\n",
    "print('#AEDECOD>=2')\n",
    "count_aeterm(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Words Shuffling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_test = df.query('target==0')\n",
    "df_train = df.query('target==1')\n",
    "# Shuffle\n",
    "texts = texts2list(df_train['AETERM'])\n",
    "decods = df_train['AEDECOD']\n",
    "\n",
    "# All shuffled words\n",
    "# shuffled = []\n",
    "# shuffled = [ list(perm) + [decod,] for text, decod in zip(texts, decods) for perm in list(permutations(text,r=MAX_WORDS))]\n",
    "\n",
    "# Random sampling from shuffled words\n",
    "MAX_SEQ = 5\n",
    "shuffled = []\n",
    "for text, decod in zip(texts, decods):\n",
    "    #Number of permuations\n",
    "    pr = scipy.special.perm(len(text), min(MAX_WORDS, len(text)), exact=True)\n",
    "    #Sampling from shuffled words\n",
    "    perms = random.sample(\n",
    "        list(permutations(text, r=min(MAX_WORDS, len(text)))), min(pr, MAX_SEQ))\n",
    "    for perm in perms:\n",
    "        shuffled.append(list(perm) + [decod,])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_train = [coder[pt[-1]] for pt in shuffled]\n",
    "y_train = to_categorical(y_train)\n",
    "x_train = [record[0:-1] for record in shuffled]\n",
    "x_train, nonnull = texts2wv(x_train)\n",
    "\n",
    "y_test = [coder[pt] for pt in df_test['AEDECOD']]\n",
    "y_test = to_categorical(y_test)\n",
    "x_test = texts2list(df_test['AETERM'])\n",
    "x_test, nonnull = texts2wv(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.layers.wrappers import Bidirectional\n",
    "from keras.layers import SimpleRNN\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "n_hidden = 100\n",
    "n_out = len(y_train[0])\n",
    "\n",
    "x_train_rnn = x_train.reshape(len(x_train), MAX_WORDS, wvsize)\n",
    "x_test_rnn = x_test.reshape(len(x_test), MAX_WORDS, wvsize)\n",
    "\n",
    "print(\"length of y_train=\", n_out)\n",
    "print(\"x_train_rnn.shape=, y_train.shape=\", x_train_rnn.shape, y_train.shape)\n",
    "print(\"x_test_rnn.shape=\", x_train_rnn.shape)\n",
    "\n",
    "\n",
    "def weight_variable(shape, name=None):\n",
    "    return np.random.normal(scale=.01, size=shape)\n",
    "\n",
    "\n",
    "# model = Sequential()\n",
    "# model.add(Bidirectional(SimpleRNN(n_hidden),\n",
    "#                         input_shape=(MAX_WORDS, wvsize)))\n",
    "# model.add(Dense(n_out, init=weight_variable))\n",
    "# model.add(Activation('softmax'))\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Bidirectional(LSTM(n_hidden), input_shape=(MAX_WORDS, wvsize)))\n",
    "model.add(Dense(n_out, init=weight_variable))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.compile(\n",
    "    loss='categorical_crossentropy',\n",
    "    optimizer=Adam(lr=0.001, beta_1=0.9, beta_2=0.999),\n",
    "    metrics=['accuracy'])\n",
    "\n",
    "# model.compile(loss='categorical_crossentropy',\n",
    "#               optimizer=SGD(lr=0.1),\n",
    "#               metrics=['accuracy'])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "es = EarlyStopping(monitor='val_loss', patience=2)\n",
    "csv_logger = CSVLogger('training.log')\n",
    "epochs = 50\n",
    "batch_size = 50\n",
    "\n",
    "hist = model.fit(\n",
    "    x_train_rnn,\n",
    "    y_train,\n",
    "    batch_size=batch_size,\n",
    "    epochs=epochs,\n",
    "    verbose=0,\n",
    "    validation_split=0.2)\n",
    "\n",
    "# plot results\n",
    "acc = hist.history['acc']\n",
    "val_acc = hist.history['val_acc']\n",
    "\n",
    "epochs = len(acc)\n",
    "plt.plot(range(epochs), acc, marker='.', label='acc')\n",
    "plt.plot(range(epochs), val_acc, marker='.', label='val_acc')\n",
    "plt.legend(loc='best')\n",
    "plt.grid()\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('acc')\n",
    "plt.show()\n",
    "\n",
    "score = model.evaluate(x_test_rnn, y_test, verbose=0)\n",
    "print('test loss:', score[0])\n",
    "print('test acc:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_in = len(x_train[0])\n",
    "n_hidden = 400\n",
    "n_out = len(y_train[0])\n",
    "dropout_rate = 0.4\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(n_hidden, input_dim=n_in))\n",
    "model.add(Dropout(dropout_rate))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(Dense(n_hidden))\n",
    "model.add(Dropout(dropout_rate))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(Dense(n_hidden))\n",
    "model.add(Dropout(dropout_rate))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(Dense(n_hidden))\n",
    "model.add(Dropout(dropout_rate))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(Dense(n_out))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# model.compile(loss='categorical_crossentropy',\n",
    "#               optimizer=SGD(lr=1),\n",
    "#               metrics=['accuracy'])\n",
    "\n",
    "model.compile(\n",
    "    loss='categorical_crossentropy',\n",
    "    optimizer=Adam(lr=0.001, beta_1=0.9, beta_2=0.999),\n",
    "    metrics=['accuracy'])\n",
    "\n",
    "es = EarlyStopping(monitor='val_loss', patience=2)\n",
    "csv_logger = CSVLogger('training.log')\n",
    "epochs = 50\n",
    "batch_size = 50\n",
    "\n",
    "hist = model.fit(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    batch_size=batch_size,\n",
    "    epochs=epochs,\n",
    "    verbose=1,\n",
    "    validation_split=0.1)\n",
    "\n",
    "#,                  callbacks=[es, csv_logger]\n",
    "\n",
    "# evaluate model\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('test loss:', score[0])\n",
    "print('test acc:', score[1])\n",
    "\n",
    "# plot results\n",
    "acc = hist.history['acc']\n",
    "val_acc = hist.history['val_acc']\n",
    "\n",
    "epochs = len(acc)\n",
    "plt.plot(range(epochs), acc, marker='.', label='acc')\n",
    "plt.plot(range(epochs), val_acc, marker='.', label='val_acc')\n",
    "plt.legend(loc='best')\n",
    "plt.grid()\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('acc')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Not shuffle words\n",
    "\n",
    "texts = texts2list(df['AETERM'])\n",
    "x_all, nonull = texts2wv(texts)\n",
    "y_all = np.asarray(\n",
    "    [coder[df['AEDECOD'][i]] for i in range(0, len(df['AEDECOD']))])\n",
    "y_all = to_categorical(y_all)\n",
    "target = df['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.layers.wrappers import Bidirectional\n",
    "from keras.layers import SimpleRNN\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "n_hidden = 200\n",
    "n_out = len(y_all[0])\n",
    "\n",
    "\n",
    "def split_data(x, y, target):\n",
    "    x_train = x[target == 1]\n",
    "    x_test = x[target == 0]\n",
    "    y_train = y[target == 1]\n",
    "    y_test = y[target == 0]\n",
    "    return x_train, x_test, y_train, y_test\n",
    "\n",
    "\n",
    "x_train, x_test, y_train, y_test = split_data(x_all, y_all, target)\n",
    "\n",
    "x_train = x_train.reshape(len(x_train), MAX_WORDS, wvsize)\n",
    "x_test = x_test.reshape(len(x_test), MAX_WORDS, wvsize)\n",
    "\n",
    "print(\"length of y_all=\", n_out)\n",
    "print(\"x_all.shape=, y_all.shape=\", x_all.shape, y_all.shape)\n",
    "print(\"x_train.shape=\", x_train.shape)\n",
    "\n",
    "\n",
    "def weight_variable(shape, name=None):\n",
    "    return np.random.normal(scale=.01, size=shape)\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Bidirectional(SimpleRNN(n_hidden), input_shape=(MAX_WORDS, wvsize)))\n",
    "model.add(Dense(n_out, init=weight_variable))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "# model = Sequential()\n",
    "# model.add(Bidirectional(LSTM(n_hidden),\n",
    "#                         input_shape=(MAX_WORDS, wvsize)))\n",
    "# model.add(Dense(n_out, init=weight_variable))\n",
    "# model.add(Activation('softmax'))\n",
    "\n",
    "# model.compile(loss='categorical_crossentropy',\n",
    "#               optimizer=Adam(lr=0.001, beta_1=0.9, beta_2=0.999),\n",
    "#               metrics=['accuracy'])\n",
    "\n",
    "model.compile(\n",
    "    loss='categorical_crossentropy', optimizer=SGD(lr=1), metrics=['accuracy'])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "es = EarlyStopping(monitor='val_loss', patience=2)\n",
    "csv_logger = CSVLogger('training.log')\n",
    "epochs = 100\n",
    "batch_size = 100\n",
    "\n",
    "hist = model.fit(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    batch_size=batch_size,\n",
    "    epochs=epochs,\n",
    "    verbose=0,\n",
    "    validation_split=0.1)\n",
    "\n",
    "# plot results\n",
    "acc = hist.history['acc']\n",
    "val_acc = hist.history['val_acc']\n",
    "\n",
    "epochs = len(acc)\n",
    "plt.plot(range(epochs), acc, marker='.', label='acc')\n",
    "plt.plot(range(epochs), val_acc, marker='.', label='val_acc')\n",
    "plt.legend(loc='best')\n",
    "plt.grid()\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('acc')\n",
    "plt.show()\n",
    "\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('test loss:', score[0])\n",
    "print('test acc:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_in = len(x_all[0])\n",
    "n_hidden = 400\n",
    "n_out = len(y_all[0])\n",
    "print(n_in, n_out)\n",
    "print(x_all.shape, y_all.shape)\n",
    "\n",
    "\n",
    "def split_data(x, y, target):\n",
    "    x_train = x[target == 1]\n",
    "    x_test = x[target == 0]\n",
    "    y_train = y[target == 1]\n",
    "    y_test = y[target == 0]\n",
    "    return x_train, x_test, y_train, y_test\n",
    "\n",
    "\n",
    "x_train, x_test, y_train, y_test = split_data(x_all, y_all, target)\n",
    "#x_train, x_test, y_train, y_test = train_test_split(x_all, y_all, test_size=0.2, random_state=0)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(n_hidden, input_dim=n_in))\n",
    "#model.add(Dropout(0.2))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(Dense(n_hidden))\n",
    "#model.add(Dropout(0.2))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(Dense(n_hidden))\n",
    "#model.add(Dropout(0.2))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(Dense(n_hidden))\n",
    "#model.add(Dropout(0.2))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(Dense(n_hidden))\n",
    "#model.add(Dropout(0.2))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(Dense(n_hidden))\n",
    "#model.add(Dropout(0.2))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(Dense(n_hidden))\n",
    "#model.add(Dropout(0.2))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(Dense(n_out))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# model.compile(loss='categorical_crossentropy',\n",
    "#               optimizer=SGD(lr=1),\n",
    "#               metrics=['accuracy'])\n",
    "\n",
    "model.compile(\n",
    "    loss='categorical_crossentropy',\n",
    "    optimizer=Adam(lr=0.001, beta_1=0.9, beta_2=0.999),\n",
    "    metrics=['accuracy'])\n",
    "\n",
    "es = EarlyStopping(monitor='val_loss', patience=2)\n",
    "csv_logger = CSVLogger('training.log')\n",
    "epochs = 100\n",
    "batch_size = 100\n",
    "\n",
    "hist = model.fit(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    batch_size=batch_size,\n",
    "    epochs=epochs,\n",
    "    verbose=0,\n",
    "    validation_split=0.1)\n",
    "\n",
    "#,                  callbacks=[es, csv_logger]\n",
    "\n",
    "# evaluate model\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('test loss:', score[0])\n",
    "print('test acc:', score[1])\n",
    "\n",
    "# plot results\n",
    "acc = hist.history['acc']\n",
    "val_acc = hist.history['val_acc']\n",
    "\n",
    "epochs = len(acc)\n",
    "plt.plot(range(epochs), acc, marker='.', label='acc')\n",
    "plt.plot(range(epochs), val_acc, marker='.', label='val_acc')\n",
    "plt.legend(loc='best')\n",
    "plt.grid()\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('acc')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_predict = model.predict(x_test)\n",
    "\n",
    "y_test_decode = [decoder[i] for i in np.where(y_test == 1)[1]]\n",
    "y_predict_decode = [decoder[i] for i in y_predict.argmax(axis=1)]\n",
    "y_decode = pd.DataFrame(\n",
    "    np.array([y_test_decode, y_predict_decode]).T, columns=['test', 'predict'])\n",
    "\n",
    "#print(y_decode[0:10])\n",
    "dfx = df[df[\"target\"] == 0].reset_index(drop=True, inplace=False)\n",
    "result = pd.concat([dfx, y_decode], axis=1)\n",
    "result['judge'] = result['AEDECOD'] == result['predict']\n",
    "result['check'] = result['AEDECOD'] == result['test']\n",
    "#result['check'].sum()\n",
    "accuracy = 100 * result['judge'].sum() / len(result)\n",
    "print('Accuracy: %.2f' % accuracy)\n",
    "summary = result.groupby('len').sum()\n",
    "summary['Acurracy'] = 100 * summary['judge'] / summary['check']\n",
    "summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "model = SVC(kernel='linear', random_state=None)\n",
    "#from sklearn.linear_model import LogisticRegression\n",
    "#model = LogisticRegression(random_state=None)\n",
    "y_train_cat = np.where(y_train == 1)[1]\n",
    "y_test_cat = np.where(y_test == 1)[1]\n",
    "model.fit(x_train, y_train_cat)\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "pred_train = model.predict(x_train)\n",
    "accuracy_train = accuracy_score(y_train_cat, pred_train)\n",
    "print('SVM training accuracy: %.2f' % accuracy_train)\n",
    "\n",
    "pred_test = model.predict(x_test)\n",
    "accuracy_test = accuracy_score(y_test_cat, pred_test)\n",
    "print('SVMtest accuracy: %.2f' % accuracy_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "forest = RandomForestClassifier(min_samples_leaf=3, random_state=0)\n",
    "forest.fit(x_train, y_train)\n",
    "\n",
    "pred_train = model.predict(x_train)\n",
    "accuracy_train = accuracy_score(y_train_cat, pred_train)\n",
    "print('RF training accuracy: %.2f' % accuracy_train)\n",
    "\n",
    "pred_test = model.predict(x_test)\n",
    "accuracy_test = accuracy_score(y_test_cat, pred_test)\n",
    "print('RF test accuracy: %.2f' % accuracy_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_train_decode = [decoder[i] for i in np.where(y_train == 1)[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.save('./data/model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "intexts = texts2list([\"Vomit a food\", \"low back pain\"])\n",
    "x_in, nonull = texts2wv(intexts)\n",
    "y_predict = model.predict(x_in)\n",
    "decoder = dict([(pt, index) for index, pt in coder.items()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "index = y_predict.argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "decoder[index[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_predict.argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "import numpy as np\n",
    "from gensim.models import Word2Vec, KeyedVectors\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from itertools import chain\n",
    "import pandas as pd\n",
    "\n",
    "wikimodel = Word2Vec.load(\"./data/wiki.en.word2vec.model100\")\n",
    "#wikimodel = KeyedVectors.load_word2vec_format(\"./data/test.model\",binary=False)\n",
    "#with open('/home/yo/PycharmProjects/coding/data/test.model','rb') as f:\n",
    "#    wikimodel = pickle.load(f)\n",
    "\n",
    "wvsize = wikimodel.vector_size\n",
    "MAX_WORDS = 5\n",
    "\n",
    "\n",
    "def is_ascii(string):\n",
    "    \"\"\"return true if non ascii characters are detected in the given string\n",
    "    \"\"\"\n",
    "    if string:\n",
    "        return max([ord(char) for char in string]) < 128\n",
    "    return True\n",
    "\n",
    "\n",
    "def texts2list(texts):\n",
    "    stop_words = set('for a of the and to in by with'.split())\n",
    "    if type(texts) == list:\n",
    "        texts = pd.Series(texts)\n",
    "    texts = texts.str.lower().str.translate(str.maketrans('()', '  '))\n",
    "    texts = texts.str.strip()\n",
    "    textlist = [[\n",
    "        word for word in document.translate(\n",
    "            str.maketrans('\\'()[],.&?\"{}-_:;', '                ')).split()\n",
    "        if word not in stop_words\n",
    "    ] for document in texts]\n",
    "    return textlist\n",
    "\n",
    "\n",
    "def texts2wv(texts):\n",
    "    dictset = list(set(chain.from_iterable(texts)))\n",
    "    wvdict = dict([(dic, wikimodel[dic]) for dic in dictset\n",
    "                   if dic in wikimodel.wv.vocab])\n",
    "    wvsize = wikimodel.vector_size\n",
    "    wv = [\n",
    "        np.array([wvdict[word] for word in text if word in wvdict.keys()])\n",
    "        for text in texts\n",
    "    ]\n",
    "    nonull = [len(vector) for vector in wv]\n",
    "    x = [\n",
    "        np.array([\n",
    "            wvdict[word] for (i, word) in enumerate(text)\n",
    "            if word in wvdict.keys()\n",
    "        ]) for text in texts\n",
    "    ]\n",
    "    x_norm = [x[i][0:MAX_WORDS] for i in range(len(x))]\n",
    "    x_all = np.array([\n",
    "        np.append(x_norm[i].flat,\n",
    "                  np.zeros(wvsize * max(MAX_WORDS - len(x_norm[i]), 0)))\n",
    "        for i in range(len(x_norm))\n",
    "    ])\n",
    "    return x_all, nonull\n",
    "\n",
    "\n",
    "from keras.models import load_model\n",
    "model = load_model('./data/model.h5')\n",
    "\n",
    "import pickle\n",
    "import os\n",
    "with open(os.path.join('.', 'data', 'coder.pkl'), 'rb') as f:\n",
    "    coder = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_predict[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(intexts)\n",
    "print(x_in)\n",
    "print(nonnull)\n",
    "print(y_predict)\n",
    "print(index)\n",
    "print(pt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wvsize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    import cPickle as pickle\n",
    "except:\n",
    "    import pickle\n",
    "\n",
    "with open('/home/yo/PycharmProjects/coding/data/wikimodel', 'wb') as f:\n",
    "    pickle.dump(wikimodel, f)\n",
    "f.closed\n",
    "\n",
    "#wikimodel.save('/home/yo/PycharmProjects/coding/data/test.model',sep_limit=100*1024**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    import cPickle as pickle\n",
    "except:\n",
    "    import pickle\n",
    "\n",
    "with open('/home/yo/PycharmProjects/coding/data/test.model', 'rb') as f:\n",
    "    wikimodel = pickle.load(f)\n",
    "\n",
    "#wvsize = wikimodel.vector_size\n",
    "\n",
    "wikimodel.vector_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('/home/yo/PycharmProjects/coding/data/test.model.bin',\n",
    "          'rb') as gcs_file:\n",
    "    test = gcs_file.read()\n",
    "from gensim.models import Word2Vec, KeyedVectors"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "toc": {
   "base_numbering": 1.0,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16.0,
    "lenType": 16.0,
    "lenVar": 40.0
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
